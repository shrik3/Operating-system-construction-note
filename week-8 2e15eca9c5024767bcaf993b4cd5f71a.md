# week-8

# **week-8**

- Course: OSC
- [Materials:](https://os.inf.tu-dresden.de/Studium/OSC/SS2022/L07-Threads.pdf)

# Kernel-Level Threads

### Approach

- One OS coroutine per application
- Application is activated by being called(preemption mechanisium)
- Coroutine swith : indirect by syscall

### Advantages:

- Independent application development
- Central scheduler implementation
- An application waiting for I/O can be “blocked” by the OS and “awakened” later
- Additional preemption mechanism can prevent CPU monopolization
    
    ![Untitled](week-8%202e15eca9c5024767bcaf993b4cd5f71a/Untitled.png)
    
- schedule (indirectly) been called by main() and starts the first application thread, does not return to main()
- resume switches from one application thread to the next.
    - the mechanism for applications to yield the CPU voluntarily
- kickoff : implemention for the OS  coroutine that starts appls,
    - runs once for each application thread
    - activates the respective application through an upcall

![Untitled](week-8%202e15eca9c5024767bcaf993b4cd5f71a/Untitled%201.png)

![Untitled](week-8%202e15eca9c5024767bcaf993b4cd5f71a/Untitled%202.png)

- What will CPU automatly stores in stack when intr comes in, in order to resume the app1 where we left off?
    - PC of the instruction where we  just interuppted at.(adr)
- problem: we have a ready list contains all coroutines we are ready to run, the process of modifying the list data structure should be seted in IH level
- solution : We need to apply interrupt synchronization for the involved data structures! →prolog epilo

- **Thread Switch in the Epilogue**
    - Scheduler data (list of ready threads) reside on the epilogue level
    - All system functions that manipulate these data must acquire the epilogue lock before (enter/leave)
        - resume must doing enter() before switch to different coroutine and in the end we need to leave to epilog level again
        - Create thread, terminate thread, voluntary thread switch, …
    - Basic rule for thread switches:
        - the yielding thread requests the lock
            - implicitly because calling resume() on the epilogue level and in the epiligue level you already have the guard locked.
        - the activated thread must release the lock
    - A application explictly calls resume, before that we say enter, then we are in the resume implementation we solved before, save all NV-Rs, switch to another stack, load all NV-Rs and then return, this return returns to kickoff function. but u still have guard locked(at the beginning of resume). in kickoff need to make sure we leave kickoff again, right before we actually called application.
    - in general : enter must corresponde leave!!

![Untitled](week-8%202e15eca9c5024767bcaf993b4cd5f71a/Untitled%203.png)

# Scheduling

## Classification

### Resource type **of the scheduled hardware resource**

- CPU scheduling
    - more process than cpu→multiplexed CPU for more than 1 P
    - waitting list requried
    - admission: waitting list
- I/O scheduling of the resource “device”, particularly “disk”
    - Device parameters and device state determine the next I/O operation
    - Scheduling decisions possibly not conforming to CPU scheduling → process with high priority cpu may end up in waiting list for long time.

### Operation mode of the controlled computer system

- Batch scheduling(in 50’ 60s)
    - for interation-less programs
    - non-preemptive scheduling
    (or preemptive scheduling with long time slices) → overhead of Context-switch minimization
- Interactive scheduling
    - for ineractiv Process
    - **Event-driven, preemptive** scheduling with short time slices
    - Partly response-time minimization by heuristics
- Real-time scheduling
    - Event- or time-driven **deterministic** scheduling
    - Guarantee of keeping environment-specific deadlines(e.g brake of the car )
    - Focus: **Timeliness**, not performance

### Point in time when the schedule is determined

- Online scheduling dynamic, **during** actual program execution
    - runs on time, soft dead line(do   have ddl but won’t be make any damage when it been skiped e,g video player)
- Offline scheduling static, **before** actual program execution
    - If complexity prohibits scheduling at runtime
        - Guarantee keeping all ddl: NP-hard
        - **Critical** if we must react to any preventable catastrophic situation
    - **Result**: Complete schedule(Schedule Table)
        - Schedule Table: which point and which task is running on the CPU, if another task is running, the schedule will forcely remove such task.
        - **(Half) automatically** generated via source-code analysis of a specialized “compiler”
            - (Half) automatically : normally for spcialize compiler you need to put some annotations, Program need to make sure those annotations are safe.(e.g loop run max 4 times → annotation : max 4 times, compiler determine maximum times of excution )
            - compiler is capable to predict how long certain piece of code in task takes.
            - Often executed by a **time-triggered** scheduler
        - Usually limited to **hard real-time** systems
            - need to make sure schedule is kept and also lots effort

### Determinism of timing and duration of process runs

- Deterministic scheduling of **known, exactly pre-computed processes**
    - **KNOWN: runtimes and ddls**
    - Exact prediction of CPU load
    - System guarantees and enforces process runtimes/deadlines
    - Time guarantees are valid regardless of system load
- Probabilistic scheduling of unknown processes
    - UNKNOWN: runtimes and ddls
    - (Probable) CPU load can only be estimated
    - System cannot give and enforce time guarantees
    - Timing guarantees conditionally achievable by application mechanisms(can only use some appl. mechanisum to keep ddls e.g temporarily raise the hierarchy)

### Cooperation behavior of (user/system) programs

- Cooperative scheduling of interdependent processes
    - Processes must **voluntarily yield the CPU** in favor of other processes
    - Program execution must (directly/indirectly) trigger **system calls**
        - directly : calling resume
        - indirectly : input/output
    - **System calls** must (directly/indirectly) activate the **scheduler(do switch)**
- Preemptive scheduling of independent processes
    - Processes are **forcibly deprived of the CPU** in favor of other processes
    - Events can trigger preemption of the running process

### Computer architecture of the system

- **Uni-processor scheduling**: **pseudo** parallel for multiprogramming/processing systems
- Multi-processor scheduling in shared-memory systems
    - Parallel process execution possible
        - All processors process **one global ready list**
            
            ![Untitled](week-8%202e15eca9c5024767bcaf993b4cd5f71a/Untitled%204.png)
            
            - **PRO**
                - Less synchronization costs
                - Better cache utilization cuz processes stay on the same CPU
            - **CON**
                - CPU can drain (empty list)
                    - Solution: On-demand load balancing **(pull)**
                        - When a READY list is empty, pull another readylist(synchronization problem again, but only once  when a cpu is drain)
                    - By a load-balancer process **(push)**
    - Each processor processes its **local ready list**
        
        ![Untitled](week-8%202e15eca9c5024767bcaf993b4cd5f71a/Untitled%205.png)
        
        - **PRO**
            - Automatic loadd balancing : No CPU runs empty
        - **CON**
            - Processes are not bound to particular CPUs
                - the data in one cache must be moved to the main mem and move to the cache of another cpu
                    - soluton: shared cache of all cpu
                - low cache utilization
            - Accesses to the READY list must be synchronized
                - Spinlock
                - Conflict probability grows with CPU count!

### Decision-making level when scheduling resources

- Long-term scheduling controls the degree of multiprogramming
    - Admission for users and processes
    - Hand over processes to medium- and short-term scheduling
- Medium-term scheduling as part of swapping
    - Move processes back and forth between RAM and disk
    - swapping: swap-out, swap-in
- Short-term scheduling schedules processes on the CPU(s)
    - Event-driven scheduling: Interrupts, system calls, signals
    - Blocking / preemption of the running process

## Scheduling Criteria

### User-oriented criteria:

- perceived system behavior
- determine user acceptance
- **Response Time**
    - Minimizing the time from a system-service request until the response, while maximizing the number of interactive processes.
- **Turnaround Time**
    - Minimizing the time between process submission and completion, i.e. the effective process runtime and all waiting times.
- **Timeliness**
    - Start and/or termination of a process at fixed points in time.
- **Determinism**
    - Deterministic execution of a process regardless of the current system load.

### System-oriented criteria

- efficient resource utilization
- determine computing costs
- **Throughput**
    - Maximizing the number of completed processes per predefined time unit. A measure for the performed “work” in a system.
    - long response time→ less context switches → decrease the overhead
- **CPU Utilization**
    - Maximizing the percentage of time the CPU executes processes, i.e. does useful work
- **Fairness**
    - Equal treatment of processes, and guarantee to schedule processes within certain time frames (no starvation).
- **Priority**
    - Executing processes with the highest (statically/dynamically assigned) priority first.
- **Load Balancing**
    - Uniform resource utilization, or prioritized execution of processes that rather seldomly allocate heavily utilized resources.

### Operating Modes and Criteria

- in general
    - Fairness
    - Load balancing
- Batch systems
    - Throughput
    - Turnaround time
    - CPU utilization
- Interactive systems
    - Response time (Proportionality – Processing time corresponds to expectation)
- Real-time systems
    - Priority
    - Timeliness
    - Determinism

## in Windows

### Processes and Threads in Windows NT

![Untitled](week-8%202e15eca9c5024767bcaf993b4cd5f71a/Untitled%206.png)

- each thread has its own set of regs and stack, all threads have global code segment and data segment
- **Process**: Environment and address space for threads
    - A Win32 process always contains at least one thread
    - **Thread**: Code-executing entity
    - Thread implementation by NT kernel
    - Scheduler assigns processing time to threads
- **The NT Scheduler**
    - Preemptive, priority-based scheduling
        - Thread with higher prio. preempts thread with lower prio.
            - regardless the thread is currently run on kernel or user mode
            - Most functionality of the Executive (“kernel”) implemented as threads, too
        - Round-Robin for threads with same priority
            - assignment of one time slice (“Quantum”)
    - Thread priorities
        - 0 to 31, subdivided in three ranges
            - Variable Priorities: 1 to 15
            - Real-time Priorities: 16 to 31
            - Priority 0 is reserved for the Zero-Page Thread
        - Threads of the Executive maximally use priority 23, with higher prio the thread will preempts kernel threads.
    - **Time Slice (Quantum)**
        
        ![Untitled](week-8%202e15eca9c5024767bcaf993b4cd5f71a/Untitled%207.png)
        
        - Quantum is decreased by 3 at every clock tick (every 10 or 15 ms) or by 1 if the thread voluntarily enters a waiting state(resume()  or yield CPU)
    - **Priority Classes, Relative Thread Priority**
        
        ![Untitled](week-8%202e15eca9c5024767bcaf993b4cd5f71a/Untitled%208.png)
        
    - Priorities: **Variable Priorities**
        - Thread priority = Process priority class + Thread priority + Boost
        - Schedler prioritize important threads
            - Quantum Streching
            - Dynamic priority boost(talk later)
        - progerss guarantee
            - every 3 to 4 seconds, up to 10 disadvantaged thread are raised to prio. 15 for 2 time slices
    
    ![Untitled](week-8%202e15eca9c5024767bcaf993b4cd5f71a/Untitled%209.png)
    
    - Priorities: Realtime Priorities
        - Pure priority-based Round-Robin
            - no progress guarantee, no dynamic boost
            - Special user privilege necessary (SeIncreaseBasePriorityPrivilege)
            - Operating system itself can be negatively affected cuz you can create a thread whith higher prio than kernel thread
        - Thread priority = REALTIME_PRIORITY_CLASS + Thread priority
        
    - Dynamic Priority Boosts
        - The system dynamically raises thread priorities in specific situations (not for REALTIME_PRIORITY_CLASS)
            - Disk input or output complete: +1
            - Mouse, keyboard input: +6
            - Semaphore, Event, Mutex: +1
            - Other events (network, pipe, …) +2
            - Event in the foreground application +2
        - Dynamic Boost gets “used up” (one level down  per Quantum been used up)
        
        ![Untitled](week-8%202e15eca9c5024767bcaf993b4cd5f71a/Untitled%2010.png)
        
    - Selecting the Next Thread (SMP)
        - goal is to achieve ”fair” Round-Robin at maximum throughput.
        - Problem: Cache effects
        - base on Affinity (mapping of CPUs to thread):
            - hard_affinity:  PIN 1 thread to a cpu
            - ideal_processor: the cpu this thread wants to run on
            - soft_affinity: recall the thread run before
            - last_run : Point in time the thread ran last
            - measure the starvation of the process
        - Algo :: CPU n calls FindReadyThread()
            - Pick highest-prioritized non-empty ready list
            - Search this ready list for a thread with
                
                ![Untitled](week-8%202e15eca9c5024767bcaf993b4cd5f71a/Untitled%2011.png)
                
            - otherwise pick head of this ready list
            
            iit has run long in last
            
            ![Untitled](week-8%202e15eca9c5024767bcaf993b4cd5f71a/Untitled%2012.png)
            
    - Changes in Windows 2003
        - One ReadyQueue per CPU,
        - Algorithm: CPU n calls FindReadyThread()
            - Pick highest-prioritized non-empty ready list of CPU n
            - Pick head of this ready list
            - If ReadyQueue completely empty, activate Idle Loop
            - In Idle-Loop: Search ReadyQueue of other CPUs

## in Linux

### Linux Tasks …

- are the Linux-Kernel abstraction for …
    - UNIX processes: one thread in one address space
    - Linux Threads: special process that shares its virtual address space with at least one other thread
- are the activities considered by the scheduler

### Linux’ Modular Scheduler

![Untitled](week-8%202e15eca9c5024767bcaf993b4cd5f71a/Untitled%2013.png)

![Untitled](week-8%202e15eca9c5024767bcaf993b4cd5f71a/Untitled%2014.png)

![Untitled](week-8%202e15eca9c5024767bcaf993b4cd5f71a/Untitled%2015.png)

# task 5

## Time slice Scheduler

- Goal: protect critical section OS using the prolog/epilogue model
- Scheduler : Timer interrupts trigger thread preemption
- Calling guard-protected methods of the scheduler:
    - Scheduler variable is now change to Guarded_Scheduler

## Preemptive Thread Switch

![Untitled](week-8%202e15eca9c5024767bcaf993b4cd5f71a/Untitled%2016.png)

## Thread Switch in epilogue

- Tips
    - Never call enter from the epilogue (double request)
    - Basic rule (see above) also holds for the first thread activation(!)
        - Kickoff function is expcected to be called with gaurd locked. so before another thread is running we need to make sure to release the guard lock.
        - similar in main() : before call scheduler.scheduler() → make sure guard is locked  before schedule is running. cuz this will run the first application and release the lock
        - **summary** : every require needs some matching release.
    - 

## Class Guarded_Scheduler

- C++ detail:
    - Because methods of Guarded_Scheduler have the same names asthose of the base class Scheduler, they hide them
    - Access hidden methods: explicitly provide **base-class scope** when calling a method
    
    ```cpp
    Guarded_Scheduler scheduler;
    Application appl1, appl2;
    scheduler.ready (appl1); // Guarded_Scheduler method
    scheduler.Scheduler::ready (appl2); // Scheduler method
    ```
    
- Programming the 8254
    
    // |   bits   |   value  |  meaning
    // |    0      |     0      |  Binary counting of 16 bits
    // |    1-3   |    010   |  Periodic interrupt
    // |    4-5   |     11   |  Low-order, then high-order counte byte
    // |    6-7   |     00   |  Counter 0
    // 7 6 5 4 3 2 1 0
    // 0 0 1 1 0 1 0 0  = 52 = 0x34
    

## Preemptive Scheduling

- Thread A uses the CPU for 18 ms and then voluntarily calls resume()
- Thread B continuously uses the CPU and never voluntarily yields

![Untitled](week-8%202e15eca9c5024767bcaf993b4cd5f71a/Untitled%2017.png)

- solution :
    - reset the timer at first resume(): Thread B get full time slice.
    - RR or VRR