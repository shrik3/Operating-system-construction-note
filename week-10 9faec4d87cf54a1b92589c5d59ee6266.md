# week-10

# **week-10**

- Course: OSC
- [Material](https://os.inf.tu-dresden.de/Studium/OSC/SS2022/L10-IPC.mp4):

# Communication and Synchronization

- If A needs a piece of information from B to continue its work, A must wait until B supplies that information.
- Message-based communication (usually) implies synchronization (e.g. in send() and receive())
- Synchronization primitives are a suitable basis for implementing communication primitives (e.g. semaphore)

# IPC via Shared Memory

## Use cases / constraints

- Unprotected system (all processes in same address space)
    - oostubs all thread resides on the same adr
- System with language-based memory protection
    - in java : enforces mem protection.
- Communication between threads in the same address space
- OS-supplied, MMU-based shared memory
- Common kernel address space of isolated processes

## Positive properties

- Atomic memory accesses do not require additional synchronization
- Fast: zero-copy
    - put a message in shared mem, it doesn’t have to copy again. cuz the receiver side simply access the data.
- Simple IPC applications easy to implement
- Unsynchronized communication possible
- multi sender : multi receiver communication simple to implement

## Semaphore – Simple Interactions

- mutual exclusion
    - make sure that access to piece of mem is usually exclusive: mutex or semaphor-like-mutex.
        - pretect all accesses frem 2 processes below to land share data structure with mutex.
            - lock mutex before we access the data structure and release after done

![Untitled](week-10%209faec4d87cf54a1b92589c5d59ee6266/Untitled.png)

- Unilateral synchronization(one side)
    - we have a queue and a semaphor to count the number of the elements within the queue
        - consumer: blocks if no ele in queue.
        - producer: increase semaphor
    
    ![Untitled](week-10%209faec4d87cf54a1b92589c5d59ee6266/Untitled%201.png)
    
    - Problem 1: put() and get() concurrently shows off.
        - adding a mutex
        
        ```cpp
        Sem m(1)
        void producer() {
        	m.wait();
        	shared.put();
        	m.signal();
        	elem.signal();
        }
        
        void consumer() {
        //Q:what happend if we add	m.wait(); here(above elem.wait();?
        //if there is no ele on the queue, the consumer 
        //will firstly lock the m and then block the elem,
        //which end up that producer() will never get change to
        //put ele into the queue
        	elem.wait();
        	m.wait();
        	shared.get();
        	m.signal();
        }
        ```
        
- Resource-oriented synchronization

![Untitled](week-10%209faec4d87cf54a1b92589c5d59ee6266/Untitled%202.png)

- Problem 2: shared data structure holds multiple element
    
    ```cpp
    Sem m(1);
    Sem free(42); //42 free slot
    
    void producer() {
    	free.wait();
    	m.wait();
    	shared.put();
    	m.signal();
    	elem.signal();
    }
    
    void consumer() {
    
    	elem.wait();
    	m.wait();
    	shared.get();
    	m.signal();
    	free.signal();
    }
    ```
    

## Semaphore – more Complex Interactions

- Readers—writers problem
    - Writers need exclusive access to memory
    - Multiple readers may work concurrently
    
    ![Untitled](week-10%209faec4d87cf54a1b92589c5d59ee6266/Untitled%203.png)
    

## Semaphore – Readers—Writers Problem

![Untitled](week-10%209faec4d87cf54a1b92589c5d59ee6266/Untitled%204.png)

- **semaphor(here reader) always block(p()) after the mutex is released!!!**
    - conditional v() before p()

```cpp
sem read(0) //initilize with 0
// Acquire (Reader)
	mutex.p();  // mutex used to protect whole CS
	ar++; // 
	if (aw==0) { //no active writer
		 rr++; 
		 read.v(); 
	}
	mutex.v(); //there is active writer, release mutex
	read.p(); //block read sem

// conditional read.v() before read.p()
// what if we reverse this 2 line?
// we will block within the CS and mutex will stay blocked.
// none of the other operations will able to comtine
```

```cpp
// Release (Reader)

mutex.p();
ar--; rr--;
while (rr==0 && ww<aw) {// no rr and at least 1 aw are not writing
	 ww++;
	 write.v();
}
mutex.v();
```

```cpp
// Acquire (Writer)
mutex.p();
aw++; // active writers
if (rr==0) {
	 ww++; // writing writers
	 write.v();
}
mutex.v();
write.p();
w_mutex.p();
```

```cpp
// Release (Writer)
w_mutex.v();
mutex.p();
aw--; ww--;
while (aw==0 && rr<ar) {
 rr++;
 read.v();
}
mutex.v()
```

- this is writers perference problem
    - if there is a aktiv reader, the writer only wait for the reading reader
- Problem: starvation problem
    - as long as there are constant writers, the readers cannever proceed.
- 写者优先:above
    - 只要有aw(write.p();), 只有之前的rr能继续进行，ar不能成为rr
- 读者优先
    - 有aw后，只要还有rr，ar就还能申请成为rr。
- 公平原则
    - 按出现的先后顺序

## Semaphore – Discussion

- Extensions
    - Non-blocking P()
    - Timeout
    - Counter array
- Sources of errors (bugs!)
    - – Semaphore use is not enforced
    - Cooperating processes depend on each other
        - All must comply with the protocol
    - Implementation effort
- Programming-language support

### Monitors – Synchronized ADTs [1]

- Idea: Couple abstract data type with synchronization
properties

![Untitled](week-10%209faec4d87cf54a1b92589c5d59ee6266/Untitled%205.png)

- Monitors – Producer—Consumer
    
    ![Untitled](week-10%209faec4d87cf54a1b92589c5d59ee6266/Untitled%206.png)
    
- Problem: while waiting condition should not holding the mutex
- Monitors –Readers—Writers
    
    ![Untitled](week-10%209faec4d87cf54a1b92589c5d59ee6266/Untitled%207.png)
    

![Untitled](week-10%209faec4d87cf54a1b92589c5d59ee6266/Untitled%208.png)

- Monitors – Discussion
    - problem:
        - - Limits concurrency to full mutual exclusion
            - only 1 process can run any mothod of object  even if produce and consume can run concurrrently, but monitor didn#t allow
            - That’s why Java allows synchronized for individual methods
        - Coupling of logical structure and synchronization not necessarily “natural”
            - see readers—writers example
            - Same problem: Just like with the semaphore, programmers must comply with a protocol

  →Synchronization should be separated from data
organization and methods.

### Path Expressions

- Idea: Flexible expressions describe permitted sequences of
execution and the object-access degree of concurrency
    - **path** name1, name2, name3 **end**
        - Arbitrary order and arbitrarily concurrent execution of name1–3
    - **path** name1; name2 **end**
        - Before each execution of name2 at least once name1
    - **path** name1 + name2 **end**
        - Alternative execution: either name1 or name2
    - **path** N:(path expression) **end**
        - max. N control flows are permitted to be in path expression
- example: **path** 10:(1:(insert); 1:(remove)) **end**
    - Synchronization of a 10-element buffer
        - Mutual exclusion during execution of insert and remove
            - only 1 process can run insert
            - only 1 process can run remove
            - remove and insert can run concurrently, but each of them cannot overlap itself
        - At least one insert before each remove
        - Never more than 10 finalized insert operations
    - implementation
        
        ![Untitled](week-10%209faec4d87cf54a1b92589c5d59ee6266/Untitled%209.png)
        
        ![Untitled](week-10%209faec4d87cf54a1b92589c5d59ee6266/Untitled%2010.png)
        
- Path Expressions – Discussion
    - PROs
        - More complex interaction patterns possible than with monitors
            - read + 1: write
        - Compliance with interaction protocols is enforced
            - Less bugs!
    - CONs
        - Synchronization behavior cannot depend on state variables or parameters
            - Extension: Path expressions with predicates
        - Synchronization of the state machine itself can become the bottleneck
        - **No support for path expressions in common programming languages**

# IPC via Messages

- Use cases/Constraints
    - IPC across machine boundaries
    - Interaction of isolated processes
- Positive properties
    - Uniform paradigma for IPC with local and remote processes
    - Buffering and synchronization if necessary
    - Indirection allows for transparent protocol extensions
    - High-level language mechanisms such as OO messages or procedure calls can be mapped to IPC via messages (RPC, RMI)

## Message-based Communication

- Already well-known from “Betriebssysteme und Sicherheit”:
Variations of send() and receive()
    - synchronous / asynchronous (blocking / non-blocking)
    - buffered / not buffered
    - direct / indirect addressing
    - fixed / variable message sizes
    - symmetric / asymmetric communication
    - with / without timeout
    - broadcast / multicast

# Basic Abstractions in Operating Systems

- Which basic IPC abstractions do operating systems offer?
    - UNIX: Sockets, System V Semaphore, messages, shared memory
    - Windows NT/2000/...: Shared memory, events, Semaphore, Mutant(mutex in user space, mutex can only been used in user space), sockets, asynchronous I/O, …
    - Mach: Messages to ports and shared memory (with copy-on-write)
- System-internal abstractions
    - Practically always: Semaphore
        - Mutual exclusion & unilateral synchronization → very common use cases
    - Microkernels and distributed operating systems: Messages
        - Basis for message implementations: Synchronization primitives
    - Monolithic systems: Semaphore and shared memory

# Duality of Concepts

- Duality – Messages in Shared Memory
- Messages are not copied
    - Sender provides memory
- Mailbox abstraction allows for M:N IPC
    
    ```cpp
    class Mailbox : public List {
     Semaphore mutex(1);
     Semaphore has_elem(0);
    public:
     void send(Message *msg) {
    	 mutex.p();
    	 enqueue(msg); // from List
    	 mutex.v();
    	 has_elem.v();
     }
     Message *receive() {
    	 has_elem.p();
    	 mutex.p();
    	 Message *result = dequeue(); // List
    	 mutex.v();
    	 return result;
     }
    };
    ```
    

![Untitled](week-10%209faec4d87cf54a1b92589c5d59ee6266/Untitled%2011.png)

- idea: Process A access the b page of the SVM, PFH figured the page is not present → PFH contact remote machine to get the page b, copies page b form right side to the left side. set right side invalid, and retry the access

![Untitled](week-10%209faec4d87cf54a1b92589c5d59ee6266/Untitled%2012.png)

### Duality – Discussion SVM

- Distributed virtual shared memory allows …
    - to apply the multiprocessor programming model on distributed systems
    - IPC via (virtual) shared memory in spite of isolated address spaces
- Problems
    - Communication and trap-handling latency
        - 2 Processes constantly access the same variable, this mem page will bounce over and back all the time, and each time it bounces it will get trap.
    - “False sharing” – Page size does not match object size
        - even if both proc didn\t access the same page, but the neighor 在同一个分区
- Approaches
    - Weak consistency models, e.g.
        - Not every access causes a trap, accept not completely updated values
        - Distribute changes asynchronously via broadcast / multicast

### Duality – Active Objects

- Objects with control flow
- Suited for access synchronization in systems with message-based IPC
- Server instantianted from aktiveObject,  aktiveObject has control flow, receive the job from client, switch is reaction of client request.

![Untitled](week-10%209faec4d87cf54a1b92589c5d59ee6266/Untitled%2013.png)

- ensure we have mutual exclusion of server side
    - we have 1 ctrl flow excutes this jobs, only one of this job is down we go back to receive next message

![Untitled](week-10%209faec4d87cf54a1b92589c5d59ee6266/Untitled%2014.png)

- make sure read and write are done concurrently
    - so doread and dowrite  actually works on a copy(fork), it doesn’t have to be atomic.

![Untitled](week-10%209faec4d87cf54a1b92589c5d59ee6266/Untitled%2015.png)

![Untitled](week-10%209faec4d87cf54a1b92589c5d59ee6266/Untitled%2016.png)

- 2 primary changes
    - no protect mutex anymore, mutex exclusion in the server
    - add message queue read and write
        - if there is currently writer, we simply put read request into this queue, once the writing is done, we dequeue the read
- Duality – Discussion
    - Is there a **fundamental difference** between IPC via shared memory and IPC via messages?
        - IPC via messages : microkernels is better, cuz they most doing message based communication
        - IPC via shared memory : monoliths is better, cuz ??
    - Example: Reader—writer monitor vs. server:
        - Monitor: 2 potential waiting points
            - Client is delayed for mutual exclusion
            - Client is potentially further delayed due to a condition variable
        - Server: 2 potential waiting points
            - Reply is delayed because the server serves other requests
            - Reply is potentially further delayed if the request must be enqueued in a waiting queue
        - Conclusion: Synchronization and concurrency identical
        
        ![Untitled](week-10%209faec4d87cf54a1b92589c5d59ee6266/Untitled%2017.png)